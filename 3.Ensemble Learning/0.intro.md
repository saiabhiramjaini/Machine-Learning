# Ensemble Learning

Ensemble learning is a technique in machine learning where multiple models (or "learners") are combined to improve overall performance. The idea is that by combining different models, you can achieve better predictive performance than any single model alone. Here’s a detailed look at the main types of ensemble learning methods:

### 1. **Bagging (Bootstrap Aggregating)**

#### **Concept:**
- Bagging involves training multiple models independently on different subsets of the training data and then aggregating their predictions. These subsets are created by randomly sampling the training data with replacement (bootstrap sampling)

![](https://github.com/user-attachments/assets/5ddcde40-7a77-4c19-8a4f-889ddd1c6ee5)

#### **Process:**
1. **Data Sampling:** Generate multiple bootstrap samples from the training dataset.
2. **Model Training:** Train a separate model on each bootstrap sample.
3. **Aggregation:** Combine the predictions of all models. For regression, this is usually done by averaging; for classification, it’s typically done by voting.

#### **Example:**
- **Random Forest:** An extension of bagging where multiple decision trees are trained on different bootstrap samples of the data. The final prediction is made by aggregating the predictions of all trees.


### 2. **Boosting**

#### **Concept:**
- Boosting involves sequentially training models where each new model corrects the errors of the previous ones. The models are trained in a weighted manner so that the errors made by earlier models are given more importance in subsequent models.

![](https://github.com/user-attachments/assets/6aca165d-5688-41dd-95e0-f00ac5a55fd0)

#### **Process:**
1. **Initial Model Training:** Train the first model on the entire dataset.
2. **Error Correction:** Adjust the weights of the training instances to emphasize the ones that were misclassified.
3. **Subsequent Models:** Train additional models focusing on correcting the errors made by the previous models.
4. **Aggregation:** Combine all models’ predictions, often by weighting them based on their accuracy.

#### **Examples:**
- **AdaBoost (Adaptive Boosting):** Adjusts weights of misclassified data points and combines multiple weak learners (usually decision trees) into a strong learner.
- **Gradient Boosting:** Builds models sequentially where each model is trained to correct the residual errors of the combined ensemble of all previous models.


### 3. **Stacking (Stacked Generalization)**

#### **Concept:**
- Stacking involves training multiple base models (level-0 models) and then combining their predictions using a meta-model (level-1 model). The meta-model learns how to best combine the base models' predictions to make the final prediction.

![](https://github.com/user-attachments/assets/be6b6bd3-32cf-4e3d-a608-1b74112cebda)

#### **Process:**
1. **Train Base Models:** Train multiple different base models on the training data.
2. **Generate Predictions:** Use these base models to make predictions on a hold-out dataset or through cross-validation.
3. **Train Meta-Model:** Train the meta-model using the predictions of the base models as features.
4. **Final Prediction:** The meta-model produces the final prediction.

#### **Example:**
- **Stacking Classifier:** Combining predictions from models such as Logistic Regression, Random Forest, and Support Vector Machines using a meta-model like another Logistic Regression.

**Code Example (Python with scikit-learn):**



### Summary

- **Bagging:** Reduces variance and helps prevent overfitting by averaging predictions from multiple models trained on different subsets of the data.
- **Boosting:** Reduces bias and improves model accuracy by sequentially focusing on correcting errors made by previous models.
- **Stacking:** Combines the strengths of different models by training a meta-model to learn the best way to combine predictions from base models.
