# Accuracy Metrics and Evaluation Measures

When building machine learning models, it is essential to evaluate their performance using appropriate metrics. Below are some of the most common evaluation metrics:

### 1. Sum of Squared Errors (SSE)
The **Sum of Squared Errors (SSE)** is a metric used to measure the total deviation of the predicted values from the actual values in regression models.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791082/render.png)

![image](https://www.sciweavers.org/upload/Tex2Img_1725791111/render.png)

#### Interpretation:
- Lower SSE indicates a better fit of the model to the data.
- SSE is sensitive to outliers because it squares the errors.



### 2. Root Mean Squared Error (RMSE)
The **Root Mean Squared Error (RMSE)** is the square root of the average squared differences between predicted and actual values.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791154/render.png)

#### Interpretation:
- RMSE is in the same units as the target variable, making it easier to interpret.
- Lower RMSE values indicate better model performance.



### 3. Coefficient of Determination (\(R^2\))
The **Coefficient of Determination (\(R^2\))** measures the proportion of variance in the dependent variable that is predictable from the independent variables.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791187/render.png)
Where:
- \( SST ) is the total sum of squares (variance of the target variable).

#### Interpretation:
- \( R^2 \) ranges from 0 to 1.
- A value closer to 1 indicates a better fit, meaning the model explains most of the variance in the target variable.



### 4. Confusion Matrix
A **Confusion Matrix** is a table used to describe the performance of a classification model. It compares the actual target values with those predicted by the model.

#### Structure:
|                | Predicted Positive | Predicted Negative |
|----------------|-------------------|-------------------|
| **Actual Positive** | True Positive (TP)   | False Negative (FN)  |
| **Actual Negative** | False Positive (FP)  | True Negative (TN)   |

#### Metrics Derived:
- **Accuracy**: ![image](https://www.sciweavers.org/upload/Tex2Img_1725791315/render.png)
- **Precision**: ![image](https://www.sciweavers.org/upload/Tex2Img_1725791350/render.png)
- **Recall (Sensitivity)**: ![image](https://www.sciweavers.org/upload/Tex2Img_1725791396/render.png)
- **F-Score**: ![image](https://www.sciweavers.org/upload/Tex2Img_1725791418/render.png)

#### Interpretation:
- Provides insight into the types of errors the model is making.
- Useful for imbalanced datasets.



### 5. Precision
**Precision** measures the proportion of true positive predictions out of all positive predictions.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791495/render.png)

#### Interpretation:
- High precision indicates a low false-positive rate.
- Useful when the cost of false positives is high (e.g., spam detection).



### 6. Recall (Sensitivity)
**Recall** measures the proportion of true positive predictions out of all actual positives.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791546/render.png)

#### Interpretation:
- High recall indicates a low false-negative rate.
- Useful when the cost of false negatives is high (e.g., disease detection).



### 7. F-Score (F1-Score)
The **F-Score** (or F1-Score) is the harmonic mean of precision and recall, providing a single metric to balance both.

#### Formula:
![image](https://www.sciweavers.org/upload/Tex2Img_1725791580/render.png)

### Interpretation:
- An F-Score close to 1 indicates good precision and recall.
- Useful when a balance between precision and recall is needed.


## 8. Receiver Operating Characteristic (ROC) Curve
The **Receiver Operating Characteristic (ROC) Curve** is a graphical representation of a modelâ€™s ability to distinguish between classes. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.
![image](https://machinelearningmastery.com/wp-content/uploads/2018/08/ROC-Curve-Plot-for-a-No-Skill-Classifier-and-a-Logistic-Regression-Model.png)
### Key Points:
- **True Positive Rate (TPR) / Sensitivity / Recall**: ![image](https://www.sciweavers.org/upload/Tex2Img_1725791643/render.png)
- **False Positive Rate (FPR)**:![image](https://www.sciweavers.org/upload/Tex2Img_1725791668/render.png)

#### Area Under the Curve (AUC):
- The **AUC** of the ROC curve represents the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative one.
- AUC ranges from 0.5 (no discrimination) to 1 (perfect discrimination).

#### Interpretation:
- A model with AUC closer to 1 is considered better.




These metrics and measures are essential tools for evaluating machine learning models. Understanding them allows you to assess model performance, choose the best model, and fine-tune it for better results.