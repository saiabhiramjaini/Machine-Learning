
# Parametric vs. Non-Parametric Machine Learning Algorithms


Machine learning algorithms can be broadly categorized into two types based on their approach to modeling: **Parametric** and **Non-Parametric**.

### Parametric Algorithms

#### Definition
Parametric algorithms assume a specific form for the function that maps inputs to outputs. They summarize the data with a set of parameters of fixed size, regardless of the amount of training data.

#### Key Characteristics
- **Fixed number of parameters**: The model complexity is limited by the number of parameters.
- **Assumption about data distribution**: Assumes a specific distribution (e.g., linear, normal).
- **Faster to learn**: Due to fewer parameters, training is generally faster.
- **Risk of underfitting**: May not capture complex patterns if the assumptions are too simplistic.

#### Examples
- Linear Regression
- Logistic Regression
- Naive Bayes
- Support Vector Machines (with linear kernel)

### Non-Parametric Algorithms

#### Definition
Non-parametric algorithms do not make strong assumptions about the form of the mapping function. They have a flexible number of parameters that grow with the training data.

#### Key Characteristics
- *&Flexible number of parameters**: The model can adapt to the complexity of the data.
- **No assumption about data distribution**: These algorithms can capture complex patterns.
- **Slower to learn**: More computationally intensive, especially with large datasets.
- **Risk of overfitting**: Can fit the training data very closely, which may not generalize well to unseen data.

#### Examples
- k-Nearest Neighbors (k-NN)
- Decision Trees
- Random Forests
- Support Vector Machines (with non-linear kernel)
- Gaussian Processes

### Comparison Table

| Feature                          | Parametric                     | Non-Parametric               |
|----------------------------------|---------------------------------|------------------------------|
| *Number of Parameters*         | Fixed                           | Variable                     |
| *Assumptions about Data*       | Strong (e.g., linearity)        | Minimal                      |
| *Complexity*                   | Lower                           | Higher                       |
| *Training Time*                | Faster                          | Slower                       |
| *Risk*                         | Underfitting                    | Overfitting                  |
| *Scalability*                  | Scales well with large datasets | May struggle with large datasets |
