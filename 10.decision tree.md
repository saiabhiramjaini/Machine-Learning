
# Decision Tree

A Decision Tree is a popular supervised learning algorithm used for both classification and regression tasks in machine learning. 

It is a tree-like structure where each internal node represents a "decision" based on a feature (attribute), each branch represents the outcome of that decision, and each leaf node represents the class label or the target value.  

![decision tree structure](https://github.com/saiabhiramjaini/portfolio/assets/115941546/11f93869-b385-48e3-8e4b-ac9c1e8b6727)

## Decision Tree Terminologies 

 - **Root Node:** Root node is from where the decision tree starts. It represents the entire data set, which further gets divided into two or more homogeneous sets. 

 - **Leaf Node:** Leaf nodes are the final output node, and the tree cannot be segregated further after getting a leaf node. 

 - **Splitting:** Splitting is the process of dividing the decision node/root node into sub-nodes according to the given conditions. 

 - **Branch/Sub Tree:** A tree formed by splitting the tree. 

 - **Pruning:** Pruning is the process of removing unwanted branches from the tree. 

 - **Parent/Child node:** The root node of the tree is called the parent node, and other nodes are called the child nodes. 


## Constructing a decision tree: (using ID3 algorithm: Iterative Dichotomiser 3) 

In ID3 algorithm for construction the decision tree there are 2 terms: 

    1. Entropy 
    2. Information gain 

**Entropy:** Entropy is a measure of impurity or disorder in a set of data. In the context of decision trees, it represents the uncertainty or randomness of the target variable for the given values of attributes. 

The entropy is maximum when the data is evenly distributed among all classes, and minimum (0) when the data is completely pure (i.e., all instances belong to the same class). 

For a dataset that has C classes and the probability of randomly choosing data from class, i is Pi. Then entropy E(S) can be mathematically represented as 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/8e593697-7e30-4b09-8b80-7ce5161e118b)
  

**Information Gain:** Information gain measures the effectiveness of an attribute in classifying the data. It quantifies the reduction in entropy (or increase in purity) that results from splitting the data based on an attribute. The attribute with the highest information gain is chosen as the splitting criterion at each step. 

Information gain is the measurement of changes in entropy after the segmentation of a dataset based on an attribute. 

Information Gain= Entropy(S)- [(Weighted Avg) *Entropy(each feature) 


## Understanding with an example

This table represents a dataset where each row corresponds to a day with various weather conditions, temperature, humidity, wind strength, and whether football was played ("Yes" or "No").


| Day | Weather | Temperature | Humidity | Wind   | Play Football? |
|-----|---------|-------------|----------|--------|----------------|
| 1   | Sunny   | Hot         | High     | Weak   | No             |
| 2   | Sunny   | Hot         | High     | Strong | No             |
| 3   | Cloudy  | Hot         | High     | Weak   | Yes            |
| 4   | Rain    | Mild        | High     | Weak   | Yes            |
| 5   | Rain    | Cool        | Normal   | Weak   | Yes            |
| 6   | Rain    | Cool        | Normal   | Strong | No             |
| 7   | Cloudy  | Cool        | Normal   | Strong | Yes            |
| 8   | Sunny   | Mild        | High     | Weak   | No             |
| 9   | Sunny   | Cool        | Normal   | Weak   | Yes            |
| 10  | Rain    | Mild        | Normal   | Weak   | Yes            |
| 11  | Sunny   | Mild        | Normal   | Strong | Yes            |
| 12  | Cloudy  | Mild        | High     | Strong | Yes            |
| 13  | Cloudy  | Hot         | Normal   | Weak   | Yes            |
| 14  | Rain    | Mild        | High     | Strong | No             |




**Step1:** Finding the root node

To find the entropy of the entire dataset based on the "Play football?" column, we calculate the probabilities for each class. Since there are only two classes, "Yes" and "No", the number of classes (c) is 2. 

The probability of "Yes" is 9/14, and the probability of "No" is 5/14. 

Using the entropy formula, we can calculate the result as shown in the image: 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/bcd2af7d-123b-44fa-b1a1-eb8c9be09e0a)


**Step 2**: Entropy of all attributes 

Let us take the case of “Sunny” there are total 5 sunny days, out of these 5 days: 

2 days correspond to "Yes" (football can be played). => probability = 2/5 
3 days correspond to "No" (football can't be played). => probability = 3/5 

Calculations are shown below: 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/0df80dd6-45cd-42c1-aca9-9952e111509e)

**Step 3:** Calculating the Information gain: 

To calculate the information gain, we subtract the total entropy from the sum of entropies of each subset weighted by their relative occurrence in the dataset. 

For example, let's consider the "Sunny" attribute. Since there are 5 instances of "Sunny" out of 14 instances in total, we multiply the entropy of "Sunny" by 5/14. This weighted entropy is then subtracted from the total entropy to determine the information gain associated with the "Sunny" attribute. 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/9fce8be9-5535-480a-a1cf-21a242eb4392)

**Step 4:** Repeat this for all the other columns: 

For Temperature: 
![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/411a241f-910c-4da3-8332-e548515e3546)

For Humidity: 
![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/627beab1-e0ac-4cb3-bb64-c22feb312db3)

For Wind: 
![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/a1db9719-9859-4e1c-ad13-380921c6998c)

**Step 5:** find out the maximum gain and assign that attribute as root node. 
![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/1689923e-f5ed-4cf5-9424-861a93a611ef)

So, here the gain of Weather is high which will be our root node 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/5c0c3615-0202-4225-934a-d3ea065ae67f)


**Finding next vertices** 

Now check whether the child nodes can be divided further or not. How to check?  

Sunny has 2 “Yes” and 3 “No” 
Cloudy has 4 “Yes” and 0 “No” 
Rain has 3 “Yes” and 2 “No”. 

In the case of "Cloudy", since there are only instances of "Yes", we can directly add a child node indicating that football is played when the weather is cloudy. This is because whenever the weather is cloudy, football is always played, and there's no need for further splitting. 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/2fcc744e-fb4e-476f-afee-e8658eb3a9fc)

To add vertices for the remaining child nodes, we follow a similar process as we did for finding the root node. However, instead of calculating the information gained with respect to the entire dataset, we calculate it with respect to the specific node or attribute under consideration. 

For example, if we are considering the node "Sunny", we calculate the entropy of the subset of data where it's sunny, and then compute the information gain based on that entropy. This helps us determine which attribute is the best to split the data further at that node. 


| Day  | Weather | Temperature | Humidity | Wind   | Play Football? |
|------|---------|-------------|----------|--------|----------------|
| Day 1  | Sunny   | Hot         | High     | Weak   | No             |
| Day 2  | Sunny   | Hot         | High     | Strong | No             |
| Day 8  | Sunny   | Mild        | High     | Weak   | No             |
| Day 9  | Sunny   | Cool        | Normal   | Weak   | Yes            |
| Day 11 | Sunny   | Mild        | Normal   | Strong | Yes            |


Node “Sunny” = For Temperature: 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/7c392f43-185d-4041-9579-71a600e6d68d)

Similarly find the Information gain for Humidity and Wind 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/3559fbbd-f35f-4dbe-bc7f-9a0a1d847dae)

Temperature has the highest information gain value. 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/3f961d5f-ff93-4ab2-bf7c-0ab3ecd72041)

We repeat this process for each remaining attribute or node to determine the best way to further divide the data and create child nodes. 

Finally, our decision tree is: 

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/16140a46-a081-4bc0-83fe-425b564852e6)

## Advantages of Decision Tree
- Easy to understand and interpret, making them accessible to non-experts.
- Handle both numerical and categorical data without requiring extensive preprocessing.
- Provides insights into feature importance for decision-making.
- Handle missing values and outliers without significant impact.
- Applicable to both classification and regression tasks.

## Disadvantages of Decision Tree
- Disadvantages include the potential for overfitting
- Sensitivity to small changes in data, limited generalization if training data is not representative
- Potential bias in the presence of imbalanced data.
