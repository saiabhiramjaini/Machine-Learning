
### Overfitting

**Definition:** Overfitting occurs when a machine learning model learns not only the underlying pattern in the training data but also noise and random fluctuations.

**Example:**
- Suppose we have a dataset of houses with features like size and number of bedrooms. A model that overfits might learn too much from the training data, capturing details specific to the training set that do not generalize well to new, unseen data.

### Underfitting

**Definitio:** Underfitting occurs when a machine learning model is too simple to learn the underlying pattern in the training data, resulting in poor performance on both the training and test data.

**Example:**
- Suppose we have a dataset of houses with features like size and number of bedrooms. A model that underfits might be too simplistic, such as a linear regression model that cannot capture the complexities of the relationship between the features and the house prices. This model would perform poorly not only on the training data but also on new, unseen data, failing to recognize important patterns and trends.


### Training, Testing, and Validation Sets

**Definition in Simple Terms:**
- **Training Set:** A subset of data used to train the machine learning model.
- **Testing Set:** A separate subset of data used to evaluate the model's performance after training.
- **Validation Set:** Optionally used to fine-tune model hyperparameters and avoid overfitting on the testing set.

**Example:**
- Splitting the house dataset into 70% for training, 20% for testing, and 10% for validation allows us to train our model on the training set, evaluate it on the testing set, and tune parameters on the validation set.


### Bias

**Definition:** Bias is the error that occurs when a model makes incorrect assumptions about the data, leading to a difference between the model's predictions and the actual values.

**Types:**
- **Low Bias:** The model makes few assumptions and fits the data well.
- **High Bias:** The model makes strong assumptions and misses important patterns in the data, leading to poor performance.

**High Bias Problems:**
- Underfitting the data.
- Poor performance on both training and test data.

**Solutions to High Bias:**
- Add more input features.
- Reduce regularization.
- Use more complex models.

### Variance

**Definition:** Variance is the error that occurs when a model is too sensitive to small fluctuations in the training data, leading to a lot of variation in the model's predictions with different training datasets.

**Types:**
- **Low Variance:** Small changes in the training data lead to small changes in the model's predictions.
- **High Variance:** Small changes in the training data lead to large changes in the model's predictions.

**High Variance Problems:**
- Overfitting the training data.
- Good performance on training data but poor generalization to test data.

**Solutions to High Variance:**
- Simplify the model.
- Reduce the number of features.
- Increase regularization.

### positives and negatives 

**1. True Positives (TP):**
- **Definition:** The instances where the model correctly predicts the positive class.
- **Example:** In a medical test for detecting a disease, if the test correctly identifies a patient who has the disease as positive, it is a true positive.

**2. True Negatives (TN):**
- **Definition:** The instances where the model correctly predicts the negative class.
- **Example:** In the same medical test, if the test correctly identifies a patient who does not have the disease as negative, it is a true negative.

**3. False Positives (FP):**
- **Definition:** The instances where the model incorrectly predicts the positive class for a negative instance.
- **Example:** In the medical test, if the test incorrectly identifies a healthy patient as having the disease, it is a false positive. This is also known as a "Type I error."

**4. False Negatives (FN):**
- **Definition:** The instances where the model incorrectly predicts the negative class for a positive instance.
- **Example:** In the medical test, if the test incorrectly identifies a patient who has the disease as negative, it is a false negative. This is also known as a "Type II error."


### Confusion Matrix

**Definition in Simple Terms:** A confusion matrix is a table that describes the performance of a classification model, showing the counts of true positives, true negatives, false positives, and false negatives.

**Example:**

we have an email spam detection system designed to classify emails as either "spam" or "not spam."

- **True Positives (TP):**
  - Emails that are spam and correctly identified as spam by the system.
  - **Example:** An email offering a suspicious prize is spam and is correctly marked as spam.

- **True Negatives (TN):**
  - Emails that are not spam and correctly identified as not spam by the system.
  - **Example:** A personal email from a friend is not spam and is correctly marked as not spam.

- **False Positives (FP):**
  - Emails that are not spam but are incorrectly identified as spam by the system.
  - **Example:** A legitimate promotional email from a trusted company is mistakenly marked as spam.

- **False Negatives (FN):**
  - Emails that are spam but are incorrectly identified as not spam by the system.
  - **Example:** A phishing email disguised as a bank notification is spam but is mistakenly marked as not spam.

|                | Predicted Positive (Spam) | Predicted Negative (Not Spam) |
|----------------|---------------------------|-------------------------------|
| **Actual Positive (Spam)** | True Positive (TP) | False Negative (FN)            |
| **Actual Negative (Not Spam)** | False Positive (FP)  | True Negative (TN)             |


### Mean Squared Error (MSE)

**Definition:** Mean Squared Error (MSE) is a commonly used metric to measure the average of the squares of the errors, which are the differences between predicted values and actual values. It quantifies how well a model's predictions match the actual data.

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/62ddc8de-0d3d-4d7f-bc8c-c2ab8cafcba2)

**Characteristics:**
- MSE gives a higher weight to larger errors due to squaring the error terms.
- It is sensitive to outliers.


### Mean Absolute Error (MAE):

**Definition:** Mean Absolute Error (MAE) measures the average magnitude of errors in a set of predictions, without considering their direction. It is the average of the absolute differences between predicted values and actual values.

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/2a853081-07b3-4822-b37d-477174f19d79)

**Characteristics:**
- MAE treats all errors equally.
- It is less sensitive to outliers compared to MSE.

### Root Mean Squared Error (RMSE):

**Definition:** Root Mean Squared Error (RMSE) is the square root of the average of the squared differences between predicted values and actual values. It provides an indication of the magnitude of error.

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/91fa0807-c4ab-40d1-87ba-4d8b150fd008)

**Characteristics:**
- RMSE is in the same units as the target variable.
- Like MSE, it is sensitive to outliers.


### Accuracy Metrics

**Evaluation Measures:**

- **Sum of Squared Errors (SSE):** Measures the sum of squared differences between predicted and actual values.
  
- **Root Mean Squared Error (RMSE):** The square root of the average of squared differences between predicted and actual values, providing a measure of prediction error.
  
- **R-squared (R2):** Indicates the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features).

**Example:**
- For our house price prediction model, SSE quantifies the total error in predictions, RMSE gives the average prediction error, and R2 indicates how well our model explains the variance in house prices.

### Precision, Recall, and F1 Score

**Definitions:**
- **Precision:** Measures the proportion of true positives among all positive predictions.
- **Recall:** Measures the proportion of true positives that were correctly identified.
- **F1 Score:** Harmonic mean of precision and recall, balancing both metrics.

**Example:**
- In a medical diagnosis scenario, precision measures how many correctly diagnosed cases are truly positive, recall measures how many positive cases were correctly identified, and F1 score combines both to assess overall model performance.

### Area Under the Curve (AUC)

**Definition:** Area Under the Curve (AUC) is a performance measurement for classification models at various threshold settings. Specifically, it refers to the area under the Receiver Operating Characteristic (ROC) curve. The ROC curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at different threshold values.

![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/d46ef605-2a09-44e2-8cf3-9062d4566397)


**Interpreting AUC:**
- **AUC = 1:** Perfect model. It means the model perfectly distinguishes between all positive and negative instances.
- **AUC = 0.5:** Model performs no better than random chance.
- **AUC < 0.5:** Model performs worse than random guessing (which could indicate an issue with the model or data).

![image](https://media.geeksforgeeks.org/wp-content/uploads/20230410164437/AUC-ROC-Curve.webp)

**Example:**
- Suppose we have a model that predicts whether an email is spam or not (a binary classification problem). The ROC curve for this model shows the trade-off between TPR and FPR at different threshold levels. The AUC value summarizes this ROC curve into a single number, indicating how well the model distinguishes between spam and non-spam emails.

In summary, AUC is a valuable metric for evaluating the performance of classification models, especially in cases where the dataset is imbalanced or the cost of false positives and false negatives varies.

#### Receiver Operating Characteristic (ROC) Curve

**Definition in Simple Terms:** A graphical plot that illustrates the performance of a binary classifier across various thresholds, showing the trade-off between true positive rate (sensitivity) and false positive rate (1 - specificity).

**Example:**
- Plotting an ROC curve for a model predicting house prices above a certain threshold shows how sensitivity (correctly predicting high prices) changes relative to specificity (incorrectly predicting high prices) as the decision threshold varies.

#### Unbalanced Datasets

**Definition in Simple Terms:** Datasets where one class (e.g., positive cases in medical diagnosis) is much more frequent than the other, leading to challenges in model training and evaluation.

**Example:**
- In fraud detection, genuine transactions are far more frequent than fraudulent ones. Training a model on such data requires handling imbalance to avoid biased predictions towards the majority class.

### Basic Statistics

#### Averages, Variance, and Covariance

**Definitions in Simple Terms:**
- **Average (Mean):** Sum of all values divided by the number of values.
- **Variance:** Measure of how spread out the values are from the mean.
- **Covariance:** Measure of how two variables change together, indicating the direction of their linear relationship.

**Example:**
- Calculating the average size of houses in a dataset provides a central tendency measure. Variance quantifies how sizes differ from this average, and covariance shows whether size and price tend to increase or decrease together.
