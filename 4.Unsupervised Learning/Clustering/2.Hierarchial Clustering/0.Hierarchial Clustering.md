

# Hierarchical Clustering

Hierarchical Clustering is an unsupervised learning algorithm used to group unlabeled datasets into clusters. This algorithm builds a hierarchy of clusters that can be visualized using a dendrogram.

## How Hierarchical Clustering Works

Hierarchical clustering works by grouping data points into a tree of clusters. The two main types of hierarchical clustering are:

1. **Agglomerative (Bottom-Up Approach):**
   - Starts with each data point as a single cluster and merges the closest pairs of clusters until all points belong to a single cluster.

2. **Divisive (Top-Down Approach):**
   - Starts with the entire dataset as one cluster and recursively splits the clusters into smaller ones until each data point is in its own cluster.

![](https://cdn.educba.com/academy/wp-content/uploads/2019/11/Hierarchical-Clustering-Analysis.png)

## Steps in Hierarchical Clustering

1. **Calculate the Distance Matrix:**
- Compute the distance between every pair of data points using a distance metric like Euclidean distance.

2. **Linkage Criteria:**
- Determine how the distance between clusters is calculated. Common linkage methods include:

    1. Single Linkage (Nearest Point Linkage)
    - Definition: The distance between two clusters is defined as the minimum distance between any single point in one cluster and any single point in the other cluster.
    - Characteristics: Can result in "chaining" where clusters can become long and straggly.
    2. Complete Linkage (Furthest Point Linkage)
    - Definition: The distance between two clusters is defined as the maximum distance between any single point in one cluster and any single point in the other cluster.
    - Characteristics: Tends to create more compact and spherical clusters compared to single linkage.
    3. Average Linkage (Mean Linkage)
    - Definition: The distance between two clusters is defined as the average distance between all points in the first cluster and all points in the second cluster.
    - Characteristics: A compromise between single and complete linkage, often resulting in clusters that are less sensitive to noise and outliers.
    4. Ward's Linkage
    - Definition: The distance between two clusters is defined as the increase in the sum of squared distances (variance) within the clusters if they were merged.
    - Characteristics: Tends to create clusters with minimal variance and is generally used for minimizing the total within-cluster variance.

3. **Merge or Split Clusters:**
   - In agglomerative clustering, merge the closest clusters step-by-step.
   - In divisive clustering, split clusters recursively.

4. **Dendrogram:**
   - Visualize the hierarchical relationship between clusters using a dendrogram. The vertical axis represents the distance or dissimilarity between clusters.

## illustration of agglomerative clustering

![Screenshot 2024-08-12 at 10 37 43 PM](https://github.com/user-attachments/assets/60b6e735-a1a4-4359-8258-2ce94a8776da)
![Screenshot 2024-08-12 at 10 37 55 PM](https://github.com/user-attachments/assets/51c1c595-e344-468f-b681-da2e1bbd61dc)
![Screenshot 2024-08-12 at 10 39 30 PM](https://github.com/user-attachments/assets/07e38071-7a52-4db9-ae6d-744019197f8a)
![Screenshot 2024-08-12 at 10 39 57 PM](https://github.com/user-attachments/assets/146ef879-811b-49fb-81e1-7cd534a1ddf9)

## Determinig number of clusters

In K-means clustering, we use elbow method for selecting the number of clusters. For Hierarchical Clustering, we use dendrogram to find the number of clusters.

The decision of the no. of clusters that can best depict different groups can be chosen by observing the dendrogram.

The best choice of the no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.

![](https://github.com/user-attachments/assets/5d76c346-ccab-44f5-9f93-d44e2988b99e)

**How we determine there are 2 clusters:**

There's a horizontal line drawn across the dendrogram at about 17.5 on the y-axis.
This line intersects with two vertical lines from the tree structure.
The text "2 clusters" is written above this line, indicating that cutting the dendrogram at this height results in 2 distinct clusters.

The decision to use 2 clusters is based on the largest vertical distance between successive merges. In this case, there's a large gap between the last merge (at the top) and the previous merges. Cutting at this point provides a natural division into two clusters:

One cluster contains P_1, P_2, and P_3
The other cluster contains P_4, P_5, and P_6

## Advantages and Disadvantages 

### Advantages:
- **No Need to Specify the Number of Clusters**: Dendrograms help in determining the number of clusters.
- **Interpretable and Intuitive**: The hierarchical structure provides insight into how clusters are formed.

### Disadvantages:
- **Computational Complexity**: Hierarchical clustering can be computationally expensive, especially with large datasets.
- **No Reassignment of Clusters**: Once a merge or split is done, it cannot be undone.

## Applications
- **Market Segmentation**: Grouping customers based on purchasing behavior.
- **Social Network Analysis**: Clustering people based on their connections and interactions.
- **Document Clustering**: Grouping similar documents based on content.
