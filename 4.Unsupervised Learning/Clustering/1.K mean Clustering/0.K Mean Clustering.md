
# K-Means Clustering Algorithm

K-Means Clustering is one of the simplest and most popular unsupervised machine learning algorithms used for clustering. It partitions the data into `k` clusters, where each data point belongs to the cluster with the nearest mean.

- The algorithm aims to minimize the within-cluster variance by iteratively assigning data points to the nearest cluster center and then recalculating the cluster centers.

- K-Means is useful for discovering groups in data where the group membership of each data point is not known.

## Distance Metrics Used in K-Means Clustering

- Euclidean Distance: Most commonly used to calculate the distance between data points and centroids.

## Steps Involved in K-Means Clustering

- **Choose the number of clusters, `k` and Initialize the centroids of the clusters randomly.**
![](https://github.com/user-attachments/assets/2c154b0d-d37e-4ca4-a8f7-0516ee14f1c0)

- **Assign each data point to the nearest centroid.**
![](https://github.com/user-attachments/assets/9d55f059-7b57-4772-98b3-dbd0c4cf5375)

- **Recalculate the centroids based on the mean of all data points assigned to each centroid.**
![](https://github.com/user-attachments/assets/7901941d-458e-452f-8af5-fa991b8e4099)

- **Repeat steps 3 and 4 until convergence (i.e., centroids no longer change).**
![](https://github.com/user-attachments/assets/9d54fa4f-0404-482b-ab2d-6e91481db44d)

![](https://github.com/user-attachments/assets/ecdadc4d-b887-435b-a75e-f436aa9fcdb2)

![](https://github.com/user-attachments/assets/92367b10-7eaf-466d-ad1d-2e55641f2038)

- **Finally we have clustered our data-points**
![Screenshot 2024-08-12 at 10 10 23 PM](https://github.com/user-attachments/assets/c5fb4021-bb4e-4e28-85da-2cdff4cab499)

### Hyperparameter `k`

The choice of `k` (number of clusters) is a hyperparameter that needs to be specified before running the algorithm. Typically, `k` is chosen based on domain knowledge or by using the Elbow Method.

### How to Choose the Value of `k` for K-Means Clustering?

The value of `k` is crucial and can be selected using methods like:

- **Elbow Method:** Plot the within-cluster sum of squares (WCSS) against the number of clusters `k` and look for an "elbow point" where the decrease in WCSS slows down.

1. **Calculating SSE**: SSE, as shown in Image, is the sum of the squared distances between each data point and its assigned cluster centroid. It's calculated as:
SSE = SSE₁ + SSE₂ + ... + SSEₖ 
![](https://github.com/user-attachments/assets/d4117f79-e4c5-4d14-a296-00d2175e8429)

2. The process involves running K-means clustering for different values of K (usually from 1 to a reasonable upper limit) and calculating the SSE for each K.

3. As K increases, the SSE generally decreases because having more clusters allows the data points to be closer to their respective centroids.

4. The "elbow point" is where the rate of decrease sharply changes, creating an elbow shape in the graph. This point represents a good balance between the number of clusters and the SSE.

5. In the Image, the elbow appears to be around K=4. After this point, the decrease in SSE is much less significant, indicating that additional clusters don't provide much better modeling of the data.
![](https://github.com/user-attachments/assets/1b965422-7144-425f-9b31-baf2e3ece5ba)

## Understanding with an Example

Suppose we have a dataset containing the height (in cm) and weight (in kg) of several individuals. The task is to group them into clusters based on these two features.

| Height (cm) | Weight (kg) |
|-------------|-------------|
| 170         | 65          |
| 160         | 58          |
| 175         | 70          |
| 155         | 54          |
| 180         | 75          |
| 165         | 60          |

### Steps

**Step 1: Choose `k` and Initialize Centroids**

Let's choose `k = 2` (we want to group the individuals into 2 clusters). We randomly initialize two centroids in the feature space.

**Step 2: Assign Data Points to Nearest Centroid**

Calculate the Euclidean distance between each data point and the centroids. Assign each data point to the nearest centroid.

**Step 3: Recalculate Centroids**

After assigning all data points to clusters, recalculate the centroids based on the mean height and weight of all data points in each cluster.

**Step 4: Repeat Until Convergence**

Continue iterating steps 2 and 3 until the centroids do not change significantly.

### Non-Deterministic Nature

K-Means is non-deterministic, meaning that it may produce different results with different initializations of centroids. Running the algorithm multiple times and choosing the best clustering solution is recommended.











