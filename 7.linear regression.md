# Linear Regression

Linear regression is a fundamental supervised machine learning algorithm used to establish a linear relationship between a dependent variable (target) and one or more independent variables (predictors) by fitting a linear equation to observed data.



## Types of Linear Regression

### Simple Linear Regression

- **Description:** Uses one independent variable to predict a dependent variable.
- **Equation:** y = Œ≤0 + Œ≤1*X
Where:
- y is the dependent variable.
- X is the independent variable.
- Œ≤0 is the intercept.
- Œ≤1 is the slope.

### Multiple Linear Regression

- **Description:** Uses more than one independent variable to predict a dependent variable.
- **Equation:**  y = Œ≤0 + Œ≤1X1 + Œ≤2X2 + ... + Œ≤n*Xn

Where:
- y is the dependent variable.
- X1, X2, ..., Xn are the independent variables.
- Œ≤0 is the intercept.
- Œ≤1, Œ≤2, ..., Œ≤n are the slopes.

### Best Fit Line

- **Objective:** To minimize the error between predicted and actual values, finding the line that best fits the data.

![image](https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png)

### Cost function for Linear Regression

The different values for weights or the coefficient of lines (Œ≤0 , Œ≤1) gives a different line of regression, so we need to calculate the best values for Œ≤0 and Œ≤1 to find the best fit line, so to calculate this we use cost function. 

In Linear Regression, the Mean Squared Error (MSE) cost function is employed, which calculates the average of the squared errors between the predicted values ùë¶ùëñ and the actual values yi.
‚Äã
### Gradient Descent for Linear Regression

A linear regression model can be trained using the optimization algorithm gradient descent by iteratively modifying the model‚Äôs parameters to reduce the mean squared error (MSE) of the model on a training dataset. 

To update Œ≤0 and Œ≤1 values in order to reduce the Cost function (minimizing RMSE value) and achieve the best-fit line the model uses Gradient Descent. 

The idea is to start with random Œ≤0 and Œ≤1 values and then iteratively update the values, reaching minimum cost. 

![image](https://www.nomidl.com/wp-content/uploads/2022/08/image-203.png)



##  Linear Regression Example: Predicting House Prices

Suppose we have a dataset of house prices and their corresponding areas (in square feet). We want to use linear regression to predict the price of a house based on its area.

### Data

| Area (sq. ft.) | Price ($) |
|----------------|-----------|
| 1200           | 250000    |
| 1500           | 320000    |
| 1800           | 370000    |
| 2000           | 415000    |

### Calculations

To find the linear regression equation, we need to calculate the slope (m) and the y-intercept (c) using the given data points.

Let:
- Y = Price
- X = Area

Calculating slope 'm':

m = (320000-250000) / (1500 - 1200) =  233.33

Calculating intercept 'c':

c = Y - mX = 250000 - 233.33 * 1200 =  -30000

### Prediction

Now, let's predict the price of a house with an area of 2200 square feet using the linear regression equation.

Substituting the values in the linear regression equation:

Y = 233.33X - 30000

Where:

X = 2200 (given area)

Y = 233.33 * 2200 - 30000

Y = 513326 - 30000

Y = 483326

So, the predicted price of a house with an area of 2200 square feet is approximately $483,326.

Note: Linear regression assumes a linear relationship between the predictor variable (area) and the target variable (price). In real-world situations, there may be non-linear relationships or other factors influencing the prices, which would require more advanced regression techniques or machine learning models.



