

# K-Nearest Neighbors Algorithm (K-NN)

K-Nearest Neighbor is one of the simplest Machine Learning algorithms based on Supervised Learning technique.

- K-NN algorithm assumes the similarity between the new case/data and available cases and puts the new case into the category that is most similar to the available categories.

- Suppose there are two categories, i.e., Category A and Category B, and we have a new data point, so this data point will lie in which of these categories. To solve this type of problem, we need a K-NN algorithm. With the help of K-NN, we can easily identify the category or class of a particular dataset. Consider the below diagram:

![K-Nearest Neighbor(KNN) Algorithm for Machine Learning](https://miro.medium.com/v2/resize:fit:600/1*WZJHcQdUyAddQz_8VS1lvQ.png)

Certainly! Let's consider another example to further illustrate the K-Nearest Neighbors (K-NN) algorithm.

## Distance Metrics Used in KNN Algorithm

- Euclidean Distance
- Manhattan Distance
- Minkowski Distance

## Understanding with Another Example

Suppose we have a dataset that contains information about smartphones, including their battery life (in hours) and screen size (in inches), which will predict their category based on usage (e.g., Gaming or Business).

| Battery Life (hours) | Screen Size (inches) | Category |
|----------------------|-----------------------|----------|
| 10                   | 5.5                   | Gaming   |
| 8                    | 5.0                   | Business |
| 12                   | 6.0                   | Gaming   |
| 9                    | 4.7                   | Business |
| 11                   | 5.2                   | Gaming   |
| 7                    | 4.5                   | Business |

Now, our task is to predict the category of a smartphone with a battery life of 9 hours and a screen size of 5.1 inches.

### Steps

**Step 1: Calculate Euclidean Distance**

Let's consider the new smartphone with \( x_2 = 9 \) (battery life) and \( y_2 = 5.1 \) (screen size). Calculate the Euclidean distance between this data point and all other data points in the dataset.

For example, calculating distances for a few data points:
![image](https://github.com/saiabhiramjaini/portfolio/assets/115941546/cc93114b-261a-47ee-b628-1ab6b5594ef8)

And so forth for all data points.

**Step 2: Select K Nearest Neighbors**

- If \( k = 1 \), we select the closest data point to the given data point, and the category of the new data point (9 hours, 5.1 inches) will be the same as the category of this nearest data point.

- If \( k = 3 \), then we need to select the 3 nearest neighbors based on the calculated distances.

**Step 3: Majority Voting**

When \( k = 3 \), suppose the 3 nearest neighbors are:
1. Smartphone 1 (10 hours, 5.5 inches) - Gaming
2. Smartphone 2 (8 hours, 5.0 inches) - Business
3. Smartphone 4 (9 hours, 4.7 inches) - Business

Now, check the majority category:
- Gaming: 1 (from Smartphone 1)
- Business: 2 (from Smartphone 2 and Smartphone 4)

Therefore, the category of the new smartphone (9 hours, 5.1 inches) based on majority voting would be **Business**.

### How to choose the value of k for KNN Algorithm?
The value of k is very crucial in the KNN algorithm to define the number of neighbors in the algorithm. 

The value of k in the k-nearest neighbors (k-NN) algorithm should be chosen based on the input data. 

If the input data has more outliers or noise, a higher value of k would be better. It is recommended to choose an odd value for k to avoid ties in classification. 

Cross-validation methods can help in selecting the best k value for the given dataset.

### Hyperparameter \( k \)

The choice of 'k' (the number of neighbors) is a hyperparameter that needs to be specified beforehand. Generally, the value of \( k \) will be 5.

### Lazy Learning Algorithm

It's a lazy learning algorithm, meaning it doesn't learn a model during training. Instead, it memorizes the training data and uses it for prediction directly.

