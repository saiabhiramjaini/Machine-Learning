
# Reinforcement Learning 

Reinforcement learning  (RL) is a machine learning technique that focuses on how an agent should take actions in an environment to maximize the total reward. The training is done in real time with continuous feedback to maximize the possibility of being rewarded. 

Reinforcement learning lets a machine learn from its mistakes, similar to how humans do. It's a type of machine learning in which the machine learns to solve a problem using trial and error. Also, the machine learns from its actions, unlike supervised learning, where historical data plays a critical role. 


### Notable characteristics of reinforcement learning (RL) are:

- Time plays a critical role in RL problems. 

- The agent's decision-making is sequential. 

- There isn't a supervisor, and the agent isn't given any instructions. There are only rewards. The agent's actions directly affect the subsequent data it receives. 

- The agent is rewarded (positive or negative) for each action. 

- The best solution to a problem is decided based on the maximum reward. 

### Terminologies used in reinforcement learning

- **Agent:** The AI system that undergoes the learning process. Also called the learner or decision- maker. The algorithm is the agent.  

- **Action:** The set of all possible moves an agent can make. 

- **Environment:** The world through which the agent moves and receives feedback. The environment takes the agent's current state and action as input and then outputs the reward and the next state. 

- **State:** An immediate situation in which the agent finds itself. It can be a specific moment or position in the environment. It can also be a current as well as a future situation. In simple words, it's the agent’s state in the environment. 

- **Reward:** For every action made, the agent receives a reward from the environment. A reward could be positive or negative, depending on the action. 

- **Policy:** The strategy the agent uses to determine the next action based on the current state. In other words, it maps states to actions so that the agent can choose the action with the highest reward. 

- **Model:** The agent's view of the environment. It maps the state-action pairs to the probability distributions over states. However, not every RL agent uses a model of its environment. 

- **Value function:** In simple terms, the value function represents how favorable a state is for the agent. The state's value represents the long-term reward the agent will receive starting from that particular state to executing a specific policy. 

- **Discount factor:** Discount factor (γ) determines how much the agent cares about rewards in the distant future when compared to those in the immediate future. It's a value between zero and one. If the discount factor equals 0, the agent will only learn about actions that produce immediate rewards. If it's equal to 1, the agent will evaluate its actions based on the sum of its future rewards. 


### Elements of reinforcement learning

Apart from the agent and the environment, there are four critical elements in reinforcement learning: policy, reward signal, value function, and model. 

1. **Policy** 

The policy is the strategy the agent uses to determine the following action based on the current state. It's one of the critical elements of reinforcement learning and can single-handedly define the agent's behavior. 

A policy maps the perceived states of the environment to the actions taken on those particular states. It can be deterministic or stochastic and can also be a simple function or a lookup table. 

2. **Reward signal** 

At each state, the agent receives an immediate signal from the environment called the reward signal or simply reward. As mentioned earlier, rewards can be positive or negative, depending on the agent's actions. The reward signal can also force the agent to change the policy. For example, if the agent’s actions lead to negative rewards, the agent will be forced to change the policy for the sake of its total reward. 

3. **Value function** 

Value function gives information about how favorable specific actions are and how much reward the agent can expect. Simply put, the value function determines how good a state is for the agent to be in. The value function depends on the agent’s policy and the reward, and its goal is to estimate values to achieve more rewards. 

4. **Model** 

The model mimics the behavior of the environment. Using a model, you can make inferences about the environment and how it’ll behave. For example, if a state and an action are provided, you can use a model to predict the next state and reward. 

Since the model lets you consider all the future situations before experiencing them, you can use it for planning. The approach used for solving reinforcement learning problems with the model’s help is called model-based reinforcement learning. On the other hand, if you try solving RL problems without using a model, it's called model-free reinforcement learning. 

While model-based learning tries to choose the optimal policy based on the learned model, model- free learning demands the agent learn from trial-and-error experience. Statistically, model-free methods are less efficient than model-based methods. 

### Reward function

In reinforcement learning (RL), a **reward function** is a crucial component that provides feedback to the agent about the quality of its actions in a given state. It helps the agent learn which actions lead to desirable outcomes and which do not. The reward function essentially defines the goal of the RL task and guides the agent's learning process.

#### Key Concepts

1. **Definition**:  
   The reward function is a mapping from state-action pairs (or just states) to numerical values, representing the immediate reward received after performing an action in a given state.

2. **Notation**:  
   Typically denoted as R(s, a) where:
   - \( s \) is the state.
   - \( a \) is the action taken in state \( s \).
   - R(s, a) is the reward received after taking action \( a \) in state \( s \).



#### Example

Consider a simple game where an agent can move in a grid to collect rewards:

- **State**: The position of the agent on the grid.
- **Action**: Move up, down, left, or right.
- **Reward Function**: \( R(s, a) \) could be:
  - +10 for reaching a goal cell.
  - -1 for moving into a cell with a penalty.
  - 0 for moving into an empty cell.

#### Designing a Reward Function

1. **Define Goals**: Determine what behaviors or outcomes are desired from the agent.
2. **Assign Rewards**: Allocate rewards that encourage these behaviors and discourage undesired actions.
3. **Test and Refine**: Iterate and adjust the reward function based on the agent's performance and learning outcomes.

#### Challenges

- **Reward Hacking**: Agents might find unintended ways to achieve high rewards, which can be mitigated by careful reward design.
- **Credit Assignment**: In environments with delayed rewards, determining which actions were responsible for achieving the reward can be challenging.

The reward function plays a central role in shaping the agent’s learning process and its ability to achieve the desired objectives in the environment.

### Discounting in Reinforcement Learning

In reinforcement learning (RL), **discounting** helps an agent decide how much to value future rewards compared to immediate rewards. This is important because rewards that come later are often less certain and harder to predict.

**Discount Factor (γ)**:  
- The discount factor, denoted as γ (gamma), is a number between 0 and 1.
  - **γ = 1**: Future rewards are valued equally to immediate rewards.
  - **γ = 0**: Only immediate rewards matter; future rewards are ignored.
  - **0 < γ < 1**: Future rewards are valued less than immediate rewards.

**Discounted Return**:  
The total reward an agent aims to maximize is called the **discounted return**. It’s calculated by giving less importance to rewards that come later. The formula is:

R(t) = r(t) + γ \* r(t+1) + γ^2 \* r(t+2) + γ^3 \* r(t+3) + ... 


Here, r(t) is the reward at time \( t \), and γ is the discount factor.

#### Example

Imagine a simple game where an agent earns rewards for collecting coins:

- **Immediate Reward**: Collecting a coin gives 10 points.
- **Future Reward**: Collecting a special coin in 3 moves gives 50 points.

If the discount factor is 0.9, the agent values the future reward (50 points) less:

- Immediate reward (at time 0): 10 points
- Reward in 3 moves (time 3): 50 points discounted by 0.9^3≈0.73 

So, the discounted future reward is about 50×0.73=36.5 points.

The agent needs to decide whether to go for the immediate coin or plan for the special coin. A higher discount factor makes the future reward more attractive, while a lower discount factor makes immediate rewards more appealing.

#### Summary

Discounting helps the agent balance between getting rewards now and waiting for potentially bigger rewards later. It helps the agent make decisions that consider both short-term and long-term benefits.

### Action Selection

**Action selection** in reinforcement learning (RL) refers to the process by which an agent chooses actions to take in an environment in order to maximize cumulative rewards. This decision-making process is crucial for the agent to learn and adapt over time. Since the agent typically doesn't know which actions will lead to the best rewards initially, it must explore the environment and use strategies to balance exploration (trying new actions) and exploitation (choosing the best-known action).

#### Epsilon-Greedy(Common Action Selection Strategies)
   - This is one of the simplest and most common action selection methods.
   - **How it works**: The agent chooses the best-known action (exploitation) most of the time, but with a small probability ϵ, it selects a random action (exploration).
   - **Formula**:
     - With probability 1 - ϵ, choose the action with the highest estimated reward.
     - With probability ϵ, choose a random action.
   - **Exploration vs. Exploitation Tradeoff**: The value of ϵ controls the tradeoff:
     - Higher ϵ: More exploration.
     - Lower ϵ: More exploitation.

#### Exploration vs. Exploitation Dilemma

- **Exploration**: Trying new actions to discover their rewards, which helps in finding better strategies but might lead to suboptimal short-term rewards.
- **Exploitation**: Choosing the best-known action based on past experience, which maximizes immediate rewards but risks missing out on better actions in the long term.

A good action selection strategy balances exploration and exploitation to ensure the agent can learn efficiently and perform well over time.

#### Example

Consider a robot that is learning to navigate a maze. Initially, it doesn’t know the best path to the exit. 

If it always follows the shortest known path (exploitation), it might miss better routes or fail to adapt to changes. On the other hand, if it tries new routes frequently (exploration), it might take too long to reach the goal. 
By using strategies like epsilon-greedy, the robot can occasionally try new paths but still stick to the best-known routes most of the time.




### Applications of reinforcement learning

- Business strategy planning 
- Aircraft control and robot motion control 
- Industrial automation 
- Data processing 
- Augmented NLP 
- Recommendation systems 
- Bidding and advertising 
- Traffic light control 
