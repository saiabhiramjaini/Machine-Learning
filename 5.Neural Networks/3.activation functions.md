
# Activation functions

### What is activation function?
An activation function in a neural network introduces non-linearity into the model, which is crucial for learning complex patterns. 

   `Neuron output = Activation function(âˆ‘(Weight * Input) + Bias)`

- **Role of Activation Functions**: They apply a non-linear transformation to the weighted sum of inputs (inputs multiplied by weights plus bias). This transformation helps decide whether a neuron should be activated or not.

- **Why Non-linearity is Important**: If we only use linear operations (multiplying inputs by weights and adding bias), the output will remain linear, regardless of the number of layers. This means the network can only learn and represent linear relationships, limiting its ability to model complex patterns.

- **Impact Without Activation Function**: Without activation functions, each neuron's output (after applying weights and biases) would be linear. This linear output would be passed to the next layer, which would also produce a linear result, and so on. As a result, no matter how many layers the network has, it would only be able to model linear relationships and would fail to capture more intricate patterns in the data.


### Types of activation functions:

Activation functions are crucial in neural networks as they introduce non-linearity, allowing the network to learn complex patterns and relationships. There are several types of activation functions, each with unique characteristics. Below are the most common types:


#### 1. **Step Function**
- **Formula**:  
![image](https://github.com/user-attachments/assets/cfc9f5f1-5804-49f3-8cc6-b378a0ea1dca)
- **Output Range**: {0, 1}

![image](https://github.com/user-attachments/assets/ae5d0477-14e4-4d22-93be-e49a93707d69)

- **Characteristics**:
  - Produces binary output (0 or 1) depending on whether the input crosses a threshold.
- **Pros**:
  - Simple and easy to implement.
  - Useful for binary classification problems.
- **Cons**:
  - Non-differentiable at x = 0, making it unsuitable for backpropagation.
  - Limited in learning complex patterns.


#### 2. **Sigmoid (Logistic) Activation Function**
- **Formula**:  
![image](https://github.com/user-attachments/assets/54812c5a-687c-43f9-a138-75ab76784726)
- **Output Range**: (0, 1)
![image](https://github.com/user-attachments/assets/de5274f3-5e16-4d11-b687-ccc389e79830)
- **Characteristics**:
  - Converts input into a value between 0 and 1.
  - Often used in the output layer of binary classification tasks.
- **Pros**:
  - Easy to understand and implement.
  - Useful for probability-based outputs.
- **Cons**:
  - Prone to **vanishing gradient** problem for very large or very small values.
  - Saturates for high input values, slowing learning.



#### 3. **Tanh (Hyperbolic Tangent) Activation Function**
- **Formula**:  
![image](https://github.com/user-attachments/assets/bde674a0-b163-45a3-b122-09b0fe108f3b)
- **Output Range**: (-1, 1)
![image](https://github.com/user-attachments/assets/fd674f73-e0e9-4ce3-92b2-3857b6344522)
- **Characteristics**:
  - Zero-centered, meaning the output is balanced around 0.
- **Pros**:
  - Works better than sigmoid for hidden layers.
  - Zero-centered outputs can help the network converge faster.
- **Cons**:
  - Still suffers from the **vanishing gradient** problem like sigmoid.



#### 4. **ReLU (Rectified Linear Unit)**
- **Formula**:  
![image](https://github.com/user-attachments/assets/a734cc90-6cce-48aa-94b9-86a6cd93cd00)
- **Output Range**: [0, infinity)
![image](https://github.com/user-attachments/assets/9ff8eb1a-d8c8-4573-98c9-7ea1a38b3f8b)
- **Characteristics**:
  - Sets all negative values to 0 while keeping positive values unchanged.
  - Most commonly used in hidden layers.
- **Pros**:
  - Computationally efficient.
  - Reduces likelihood of **vanishing gradient**.
- **Cons**:
  - Can lead to the **dying ReLU** problem, where neurons become inactive and stop learning if they receive negative inputs.



#### 5. **Leaky ReLU**
- **Formula**:  
![image](https://github.com/user-attachments/assets/42592177-05ed-4f69-b290-8df22532ef00)
  where alpha is a small constant (e.g., 0.01).
- **Output Range**: (-infinity, infinity)
![image](https://github.com/user-attachments/assets/5549f0b6-a4d4-42b3-936f-c6a4381c65cc)
- **Characteristics**:
  - Similar to ReLU, but allows small, non-zero gradient for negative inputs.
- **Pros**:
  - Solves the **dying ReLU** problem by allowing small negative values to flow.
- **Cons**:
  - The choice of \(\alpha\) is arbitrary and can impact performance.



#### 6. **Softmax Activation Function**
- **Formula**:  
![image](https://github.com/user-attachments/assets/e1e6e6ae-61b8-494d-b88a-07d07c240b7f)
- **Output Range**: (0, 1), outputs sum to 1.
![image](https://github.com/user-attachments/assets/05665e57-fe5e-43f9-8642-e1fc225ea253)
- **Characteristics**:
  - Used in the output layer for multi-class classification problems.
  - Converts raw output scores into probabilities for each class.
- **Pros**:
  - Provides a clear probability distribution across multiple classes.
- **Cons**:
  - Expensive to compute due to exponentials.
  - Can saturate for large input values.


| **Activation Function** | **Range**     | **Pros**                                    | **Cons**                                |
|-------------------------|---------------|---------------------------------------------|-----------------------------------------|
| **Step Function**        | \( \{0, 1\} \)  | Simple, binary output                      | Non-differentiable, limited learning    |
| **Sigmoid**              | \( (0, 1) \)   | Easy to implement, useful for probabilities | Vanishing gradient, slow learning       |
| **Tanh**                 | \( (-1, 1) \)  | Zero-centered, faster convergence           | Vanishing gradient                      |
| **ReLU**                 | \( [0, \infty) \) | Efficient, reduces vanishing gradient      | Dying ReLU                              |
| **Leaky ReLU**           | \( (-\infty, \infty) \) | Solves dying ReLU                          | Arbitrary \(\alpha\) choice             |
| **Softmax**              | \( (0, 1) \)   | Converts scores to probabilities            | Computationally expensive               |


Each activation function has its own advantages and is suited for specific tasks. In practice, **ReLU** and its variants are widely used in hidden layers, while **Softmax** is commonly applied in the output layer for multi-class classification.

