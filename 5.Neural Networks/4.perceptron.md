# Perceptron


### What is a perceptron?

A perceptron is a single-layer neural network. It's the simplest form of a neural network, where there is only one layer of neurons (also called nodes) between the input and the output.

Structure of a Single-Layer Perceptron:
- **Input Layer**: The perceptron receives input features (like pixel values, words, or other data points). This input layer doesn't do any computations; it simply passes the data to the next layer.

- **Output Layer**: The perceptron has just one layer of computational neurons. Each neuron in this layer takes the weighted sum of the inputs, adds a bias, and passes the result through an activation function (typically a step function) to make a binary decision (like yes/no or true/false).

**A perceptron uses step function to process the input**

![image](https://datascientest.com/en/files/2021/04/illu_perceptron_blog-138.png)

Because it only has one layer of neurons (the output layer), the perceptron is considered a single-layer neural network.

**Drawback**: Can only learn and process linear data.


### What is a multi layer perceptron?

A **Multilayer Perceptron (MLP)** is a more advanced version of the basic perceptron. It consists of multiple layers of neurons, which makes it capable of solving more complex problems, especially those that are **non-linearly separable** (where the data cannot be separated by a straight line).

**Structure of a Multilayer Perceptron:**
1. **Input Layer**: This is where the MLP receives the input data. The input layer doesn't perform any computations; it simply passes the data to the next layer.

2. **Hidden Layers**: These are layers of neurons between the input and output layers. The **hidden layers** are where most of the processing happens. Each neuron in a hidden layer performs a computation based on its inputs, weights, biases, and activation function. MLPs typically have one or more hidden layers, which allow the network to learn complex patterns and relationships in the data.

3. **Output Layer**: The final layer that gives the prediction or classification based on the processed data from the hidden layers. Depending on the problem, this layer could have one or more neurons.

**Key Features of MLP:**
- **Multiple Layers**: MLPs have at least one hidden layer, unlike the basic perceptron which only has a single layer of neurons. This makes MLPs capable of learning more complex patterns.
- **Non-Linear Activation Functions**: Unlike a simple perceptron that uses a step function, MLPs typically use non-linear activation functions like:
  - **ReLU (Rectified Linear Unit)**: Outputs the input if itâ€™s positive, otherwise outputs 0.
  - **Sigmoid**: Squeezes the output between 0 and 1.
  - **Tanh**: Squeezes the output between -1 and 1.
  
  These non-linear activation functions allow the network to learn from non-linear data.

- **Backpropagation**: The MLP learns through a process called **backpropagation**, where the network adjusts its weights and biases based on the error (or loss) in its predictions. This helps the network improve over time by minimizing the error.

**How It Works:**
1. **Feedforward**: The input data passes through the network, moving from the input layer to the hidden layers, and finally to the output layer. Each neuron in the hidden layers processes the data by applying weights, biases, and activation functions.
   
2. **Error Calculation**: The output of the MLP is compared with the true label (ground truth) to calculate the error or loss.

3. **Backpropagation**: The error is sent back through the network, and the weights and biases are adjusted to reduce this error in the next iteration. This process is repeated for multiple iterations (epochs) to improve accuracy.

**Applications of MLP:**
- **Image Recognition**
- **Speech Recognition**
- **Natural Language Processing**
- **Financial Forecasting**

