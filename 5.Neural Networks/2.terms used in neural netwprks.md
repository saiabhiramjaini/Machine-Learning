




### What is a Bias? 

Bias can be defined as the constant which is added to weights of neurons. It shifts the activation function towards the positive or negative side. 

 

### What is forward propagation? 

Forward propagation in neural networks refers to the process of computing the output of the neural network for a given input. It involves the following steps: 

- Passing the input through each layer of the neural network. 

- Computing the weighted sum of inputs at each neuron. 

- Applying the activation function.  

- Propagating the computed values forward until reaching the output layer. 

 

### What is backward propagation? 

Backward propagation, also known as backpropagation, is the process used in training neural networks to update the model's parameters (weights and biases) by computing the gradients of the loss function with respect to these parameters. It's an essential step in the training process that enables the network to learn from the errors made during forward propagation. 



### What is Cost function/Loss function?  

Cost functions, also known as loss functions, are mathematical functions that measure how well a machine learning model, like a neural network, is performing. They quantify the difference between the predicted output of the model and the actual target values (ground truth). The goal during training is to minimize the cost function, leading to better predictions.


Here are the common types of cost functions used in machine learning:

1. Mean Squared Error (MSE)
Formula:
![image](https://github.com/user-attachments/assets/809cbfc1-b17b-4c73-ab04-99d1a4a425e9)
n is the number of samples.
- Use case: Typically used for regression problems, where the goal is to predict continuous values.
- Explanation: MSE calculates the average of the squared differences between the actual and predicted values. Squaring the differences ensures that larger errors are penalized more heavily.
2. Mean Absolute Error (MAE)
Formula:
![image](https://github.com/user-attachments/assets/07764e97-13bf-4a27-9041-a19b80cadee0)
- Use case: Also used for regression problems.
- Explanation: Unlike MSE, MAE takes the absolute difference between the actual and predicted values, making it less sensitive to outliers. It gives a more direct measure of the average prediction error.
 

### What are optimizers? 

Optimizers are algorithms or methods used in deep learning to update the parameters (weights and biases) of a model during training process.  

Their main goal is to minimize the loss function and find the optimal set of parameters that best fit the training data. 

Optimizers adjust the parameters based on the gradients of the loss function with respect to those parameters. 

![image](https://miro.medium.com/v2/resize:fit:1200/1*ZXAOUqmlyECgfVa81Sr6Ew.png)


### What is Gradient Descent?

Gradient Descent is an optimization algorithm used to minimize the cost function. The idea is to iteratively adjust the parameters (weights and biases) of the neural network in the direction that reduces the cost function.
In each step, the network computes the gradient (partial derivative) of the cost function with respect to each parameter. This gradient tells us how much the cost will change if we adjust the parameter slightly.

The parameters are then updated using the formula:
```
  θ=θ−η⋅∇J(θ)
```
where:
- θ represents the network's parameters (weights and biases),
- η is the learning rate,
- ∇J(θ) is the gradient of the cost function with respect to the parameters.
![image](https://cdn.analyticsvidhya.com/wp-content/uploads/2024/09/631731_P7z2BKhd0R-9uyn9ThDasA.webp)

The **learning rate** `η` controls how much the parameters are adjusted in each iteration. It is a hyperparameter that dictates the step size during gradient descent.
- Small learning rate: The network will converge more slowly, but it is less likely to overshoot the minimum.
- Large learning rate: Faster convergence, but it can cause the network to miss the minimum by taking steps that are too large, possibly leading to instability.
![image](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)