
# Radial Basis Functions

In ML, data can be either linearly separable or non linearly separable.

![image](https://github.com/user-attachments/assets/00ee63db-4d44-4a53-be71-d06a31605bf3)

Single layer perceptron can be used for classifying linearly separable data.

Multi layer perceptron is required for classifying non linearly separable data.

**Radial Basis Function is a type of Multi Layer Perceptron which has one input layer, one output layer and with strictly one hidden layer.**

The hidden layer uses a non-linear radial basis function as the activation function, which converts the input parameters into high dimension space which is then fed into the network to linearly separate the problem.

![image](https://miro.medium.com/v2/resize:fit:713/1*88nzaKMHyjTHaoYIr8shzA.png)


### How an RBF Network Works (Step-by-step)
In an **RBF Network**, the hidden layer uses **Radial Basis Functions** (RBFs) to transform the input data. The **Gaussian function** is the most commonly used RBF in this hidden layer. Here's how it works step by step:

#### 1. Input to the Hidden Layer
When an input feature vector `x` is passed to the hidden layer, each hidden neuron computes the distance between `x` and a predefined or learned **center** `x`. This center can be thought of as a "prototype" or reference point for the data.

#### 2. The Gaussian Function
The hidden layer applies a **Gaussian function** (or another RBF) to the computed distance. The Gaussian function is defined as:

![image](https://github.com/user-attachments/assets/ebcd53a4-26b8-4781-91d7-e04911976608)

Where:
- r is the **Euclidean distance** between the input `x` and the center `ci` of the hidden neuron:
```
r = || x-ci ||
```
  
- ùúé is a **parameter** that controls the spread or width of the Gaussian function (it determines how "sensitive" the neuron is to inputs that are far from the center).
- `exp` is the exponential function, which makes the output decay as the distance increases.

#### 3. Interpretation of the Gaussian Function

- **When the input `x` is close to the center `ci`** : 
  The distance `r` is small, and the value of the Gaussian function œï(r) is close to 1. This means the neuron strongly "activates" or responds.
  
- **When the input `x` is far from the center `ci`**:
  The distance \( r \) is large, and the value of the Gaussian function  œï(r) approaches 0. This means the neuron has little or no response to the input.

#### 4. Output of the Hidden Layer
Each hidden neuron outputs a value based on the Gaussian function applied to the distance from its center. These outputs are **non-linear** transformations of the input data. The closer an input is to the neuron's center, the higher the activation. Each hidden neuron will respond strongly to inputs near its center and weakly to inputs farther away.

#### 5. Combining Hidden Layer Outputs
The outputs from all the hidden neurons are then passed to the **output layer**, where they are combined linearly to produce the final result (for classification or regression).



### **Example**
Suppose you have two hidden neurons with centers `c1` and `c2`:

1. The input vector `x` arrives at the hidden layer.
2. Neuron 1 calculates the Euclidean distance 
```
r1=||x‚àíc1|| 
```
and applies the Gaussian function:
![image](https://github.com/user-attachments/assets/d23b0241-a144-4930-a1ca-b30ecf776643)

3. Neuron 2 calculates the Euclidean distance 
```
r2=||x‚àíc2|| 
```
and applies the Gaussian function:
![image](https://github.com/user-attachments/assets/970411e1-d8e0-44e3-9bf8-71f44c0fcbe7)

4. The outputs `œï1` and `œï2` are passed to the output layer, where they are combined to generate the final output.


### **Summary**
In the hidden layer of an RBF Network, each neuron applies a **Gaussian function** to the distance between the input and its center. This creates a **localized response** where the neuron activates strongly for inputs close to its center and weakly for inputs far away. This non-linear transformation of the input helps in solving complex, non-linearly separable problems.

