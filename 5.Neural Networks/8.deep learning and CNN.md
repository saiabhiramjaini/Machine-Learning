# Deep Learning

Deep learning is a type of machine learning that mimics the way our brain works to solve
complex problems. It involves training artificial neural networks, which are computational
models inspired by the human brain's structure and function.

In simple terms, deep learning works by feeding a computer system with a large amount of data and letting it learn from that data on its own. The system automatically discovers
patterns, features, and relationships in the data, without explicitly being programmed.
These patterns and features are then used to make predictions, classify objects, or solve
other tasks.

Deep learning is called "deep" because it involves multiple layers of interconnected
artificial neurons. Each layer learns to recognize different levels of abstraction and
complexity in the data. The output from one layer becomes the input for the next layer,
allowing the network to learn progressively more sophisticated representations of the data.

**Example**: If we want a computer to recognize a face then we need to code that there are eyes, nose, mouth etc for a face but the code becomes highly complex and nearly can‚Äôt be
done. So now with the help of deep learning what we do is we feed a bunch of data for the computer in form of images and now deep learning algorithm will train the data by understanding the given images (feature extraction). Now if we give a new face to it, deep learning model will detect the face easily.


## Convolutional Neural Network (CNN)
Humans are able to detect things around them due to their ability of vision, now  
what about computers? 
How can they detect things around them?  
The answer is through CNN. 
So through CNN we provide vision to computer.
Firstly let us understand how does a computer see images :
For a computer ‚Äúimages are numbers‚Äù i.e. an image is just a matrix of pixels.

![image](https://github.com/user-attachments/assets/439f833a-9e7a-45fa-acdd-d80ecdd6986e)

If we were to use a **neural network** for object detection instead of a CNN, consider an image with a resolution of 1920 x 1080 x 3 (where 3 represents the RGB channels‚Äîone each for red, green, and blue).

- The input layer would have 1920 * 1080 * 3, or approximately 6 million neurons.
- Let's assume the hidden layer has around 4 million neurons.
- The number of weights between the input and hidden layer would be 6 million * 4 million = 24 million.

So, just between the input and hidden layer, you'd need to calculate 24 million weights. Since deep learning models often have multiple hidden layers, the total number of weights can quickly reach 500 million or even 1 billion, resulting in extremely high computational costs.

**Disadvantages of using a neural network for image classification**:
- **Excessive computational complexity**: The large number of parameters makes training very slow and resource-intensive.
- **Ignores spatial structure**: Neural networks treat all pixels equally, meaning nearby pixels and distant pixels are processed the same way, which makes it hard to capture important local patterns in images.

This makes standard neural networks inefficient for image classification tasks, especially when compared to CNNs, which are optimized for such tasks by reducing the number of parameters and capturing spatial information effectively.


### How does a CNN work?

**Convolutional Neural Networks (CNNs)** are a specialized type of artificial neural network primarily used for processing structured grid-like data, such as images. 

CNNs are particularly effective for tasks like image classification, object detection, and face recognition because they capture spatial hierarchies and local patterns in data. Below is a detailed explanation of CNNs, including their architecture and how they work:

A CNN is composed of several layers, each with a specific role. These layers are:

#### a. **Convolutional Layer (Conv Layer)**
The convolutional layer is the core building block of a CNN. It performs convolution operations on the input image by applying filters (also called kernels) to extract features like edges, textures, and patterns.

- **Filter/Kernels**: A small matrix (e.g., 3x3 or 5x5) that slides across the input image to detect features.
- **Convolution Operation**: Each kernel multiplies its values with the input pixels and sums the result. This produces a feature map.
- **Stride**: Determines how much the filter shifts as it slides over the input. A stride of 1 moves one pixel at a time.
- **Padding**: Sometimes, the image borders are padded with zeros to control the size of the output feature map.


#### b. **Activation Function (ReLU)**
After the convolution, an activation function is applied to introduce non-linearity. The most commonly used activation function is **ReLU (Rectified Linear Unit)**.

- **ReLU**: It replaces all negative pixel values in the feature map with zero, which helps the network learn complex patterns without increasing the computational load.

#### c. **Pooling Layer**
The pooling layer is used to reduce the spatial dimensions (width and height) of the feature maps, lowering the number of parameters and computations in the network.

- **Max Pooling**: The most common pooling operation. It selects the maximum value from a defined window (e.g., 2x2) and downsamples the feature map. This helps in making the network more robust to small translations in the input (i.e., small shifts in the image won‚Äôt affect the results).

#### d. Flattening
Flattening transforms the 3D feature maps into a 1D vector that can be fed into fully connected layers for final decision making.

#### e. **Fully Connected Layer (FC Layer)**
Once the convolutional and pooling layers have extracted the features, the fully connected layers take over. The output of the final pooling layer is flattened into a 1D vector, which is then passed through one or more fully connected layers.

- These layers compute a final decision based on the features extracted by previous layers and perform classification.
- The fully connected layer is where high-level reasoning takes place, and the final output is typically a vector of probabilities for each class (e.g., cat, dog, etc.).


![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkQ0hXDaQv57sALXAJquxA.jpeg)

### 1. Convolution Layer

The **convolutional layer** is the fundamental building block of a Convolutional Neural Network (CNN) and is responsible for detecting features in the input data, such as edges, textures, or more complex patterns. Below is a detailed explanation of how the convolutional layer works, including key concepts like filters, stride, padding, and how feature maps are created.

#### What is Convolution?
Convolution is a mathematical operation that combines two functions to produce a third function. In the context of CNNs, the convolutional operation applies a small matrix, called a **filter** or **kernel**, to the input image (or feature map) to produce an output, known as the **feature map** or **activation map**. This helps the network learn important patterns and features from the input data.

#### Components of the Convolutional Layer:

#### a. **Filters (Kernels)**
- A **filter** is a small matrix (e.g., 3x3, 5x5) with learnable weights.
- Each filter scans across the input image (or the feature map from a previous layer) to detect specific features.
- The number of filters in a convolutional layer is typically set by the user and determines the number of output feature maps.

Each filter detects a different feature in the image. For example:
- One filter might detect vertical edges.
- Another might detect horizontal edges.
- Deeper layers may detect complex features like curves, textures, or even entire objects.


#### b. **Stride**
- **Stride** refers to how much the filter moves (or "slides") across the input image.
- A **stride of 1** means the filter moves one pixel at a time across the image.
- A **stride of 2** means the filter jumps two pixels at a time, which results in a smaller output feature map because fewer positions are sampled.

Smaller strides preserve more spatial information but increase the size of the output, while larger strides reduce the size of the feature map and the computational load but may lose detail.

#### c. **Padding**
- **Padding** is used to control the size of the output feature map.
- Without padding, the output feature map will be smaller than the input because the filter cannot fit into the pixels at the border of the image. This is called **valid padding**.
- **Zero-padding** (or just "padding") is the practice of adding extra pixels (usually filled with zeros) around the border of the input image to ensure the filter can apply convolution to the border pixels as well. This allows the output to have the same spatial dimensions as the input.

Padding ensures that the information from the borders of the image is not lost and helps maintain the spatial size of the feature map.

![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O06nY1U7zoP4vE5AZEnxKA.gif)

![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bmx3AHBhx0D5OmBSxtY6gw.gif)

![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X22-wmPcir4y5VoeqDyvWg.gif)


### 2. Activation function

The **activation function layer**, particularly the **ReLU (Rectified Linear Unit)** function, plays a crucial role in CNNs by introducing non-linearity to the model. This non-linearity enables the network to learn and model more complex patterns and representations in the data.

#### Why Use Activation Functions?

In any neural network, including CNNs, the output of a neuron is a weighted sum of inputs followed by the application of an **activation function**.  
Without activation functions, the network would behave like a simple linear transformation, regardless of the depth of the network.  
Linear transformations are limited because they cannot capture the complex relationships in data (like patterns in images). Activation functions allow the network to learn these non-linear relationships, making it powerful enough to model real-world data.


#### ReLU (Rectified Linear Unit)
The ReLU function is the most commonly used activation function in CNNs. Its formula is simple:
```
ReLU (ùë•) = max(0,ùë•)
```

This means:

- If the input is positive, ReLU returns the same value.
- If the input is negative, ReLU returns 0.

![image](https://github.com/user-attachments/assets/e684f493-12e9-4cdc-9b21-a8850838651d)


### 3. Pooling Layer

The **pooling layer** in a CNN is used to reduce the spatial dimensions (width and height) of the input feature maps, while preserving important information. It plays a crucial role in making the network computationally efficient and less prone to overfitting. Pooling helps downsample the data, reducing the number of parameters and computations in the network.

#### Why Use Pooling Layers?

- **Dimensionality Reduction**: Pooling reduces the size of the input data, which in turn reduces the computational load on the network.
- **Preserving Key Features**: Even though pooling reduces the data size, it retains the most important features in the input. 
- **Invariance to Minor Variations**: Pooling makes the network more robust to small changes or distortions in the input, such as slight shifts, rotations, or zooms in the image.

#### Types of Pooling

##### a. Max Pooling
The most common type of pooling in CNNs is **max pooling**, which takes the maximum value from each window of the feature map. It is defined by a filter size and stride, similar to the convolutional layer.
![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vOxthD0FpBR6fJcpPxq6Hg.gif)

##### b. Average Pooling
**Average pooling** takes the average of all values in a window, rather than the maximum. While less common in practice than max pooling, it can be useful in certain tasks where the average feature representation is more meaningful than the maximum.
![image](https://miro.medium.com/v2/resize:fit:1026/format:webp/1*qWygChov3JMLTV53C4hRhw.png)

### 4. Flattening

**Flattening** is an essential step before passing the output from the convolutional or pooling layers to the fully connected layers in a CNN. Here's why and how it's done:

### What is Flattening?

**Flattening** is the process of converting a multidimensional array (like the feature map output from convolutional or pooling layers) into a single, one-dimensional vector. This step is necessary because the fully connected layer expects a 1D input vector, not a 2D or 3D array, which is typically the output of previous layers.

### Why is Flattening Needed?

In CNNs, the convolutional and pooling layers preserve the spatial structure of the data (e.g., height, width, and depth of the image), but fully connected layers are designed to work with a 1D vector. Flattening bridges this gap by reshaping the 3D output (height √ó width √ó channels) into a single vector that can be fed into the fully connected layers.

After flattening, the 1D vector is passed to the fully connected layer(s), which then perform classification or regression tasks based on this vector representation of the image's features.

![image](https://github.com/user-attachments/assets/1a99c309-09ff-470a-8a02-fca7037a5f93)


### 5. Fully Connected Layer

The **fully connected layer (FC layer)** in a CNN connects every neuron in the current layer to every neuron in the next layer. It plays a critical role in combining the features extracted by previous layers to perform the final classification or regression task. This layer is typically located near the end of the CNN architecture.

#### Why Use Fully Connected Layers?

- **Combining Features**: After feature extraction through convolutional and pooling layers, the fully connected layer uses these features to make final decisions.
- **Decision Making**: It helps in mapping the high-level features learned from the input to specific output classes, such as detecting the presence of objects in an image.

#### How It Works

In a fully connected layer:
- All neurons from the previous layer are connected to every neuron in the current layer.
- These connections are associated with weights and biases, which are updated during the training process.

The output of a fully connected layer is computed by:
- Taking the weighted sum of all inputs.
- Passing it through an activation function (often ReLU or Softmax for classification tasks).

#### Activation in Fully Connected Layer

**Softmax Activation:** In the final fully connected layer for classification tasks, Softmax is used to convert raw scores into probabilities for each class.

Fully connected layers are used in the final stages of most CNNs for tasks like image classification, object detection, and more.


![image](https://indiantechwarrior.com/wp-content/uploads/2023/08/Hidden-Layer-1-1024x603-min.jpg)


### Final CNN architecture

![image](https://editor.analyticsvidhya.com/uploads/29624cnn_banner.png)
